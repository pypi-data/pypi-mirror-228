from __future__ import annotations

import typing

import flood

if typing.TYPE_CHECKING:
    import nbformat


def create_load_test_report(
    test_paths: typing.Sequence[str],
    output_dir: str,
    metrics: typing.Sequence[str] | None = None,
) -> None:
    import os

    ipynb_path = os.path.join(output_dir, 'report.ipynb')
    _create_load_test_report_ipynb(
        test_paths=test_paths,
        output_path=ipynb_path,
        metrics=metrics,
    )

    _create_load_test_report_html(ipynb_path=ipynb_path)


def print_load_test_summary(test: flood.LoadTest) -> None:
    import toolstr

    parsed = flood.user_io.parse_test_data(test)
    rates = parsed['rates']
    durations = parsed['durations']
    vegeta_args = parsed['vegeta_args']
    styles = flood.user_io.styles

    toolstr.print_bullet(key='sample rates', value=rates, styles=styles)
    if len(set(durations)) == 1:
        toolstr.print_bullet(
            key='sample duration',
            value=durations[0],
            styles=styles,
        )
    else:
        toolstr.print_bullet(
            key='sample durations', value=durations, styles=styles
        )
    if vegeta_args is None or len(vegeta_args) == 0:
        toolstr.print_bullet(key='extra args', value=None, styles=styles)


def _create_load_test_report_html(
    ipynb_path: str,
) -> None:
    import subprocess

    cmd = 'python3 -m nbconvert --to html --execute ' + ipynb_path
    subprocess.call(cmd.split(' '))


def _create_load_test_report_ipynb(
    test_paths: typing.Sequence[str],
    output_path: str,
    metrics: typing.Sequence[str] | None = None,
) -> None:
    from flood.user_io import notebook_io

    if metrics is None:
        metrics = ['success', 'throughput', 'p50', 'p90', 'p99']

    test_paths_map = {}
    for test_path in test_paths:
        payload = flood.load_single_run_test_payload(test_path)
        name = payload['name']
        test_paths_map[name] = test_path

    inputs = {
        'test_paths': test_paths_map,
        'metrics': metrics,
        'test_names': list(test_paths_map.keys()),
    }

    notebook_io.create_notebook(
        cell_templates=_report_template_cells,
        output_path=output_path,
        inputs=inputs,
    )


_header_template = """# `flood` Load Test Report


{toc}

### Report Generation

*This report was generated by `flood`. See the `flood report --help` command for report generation options. This report can be executed as a Python notebook using the `.ipynb` version of this file.*"""  # noqa: E501


_toc_template = """### Contents
1. [Test Summary](#Test-Summary)
2. [Tests](#Tests)
{toc_lines}"""


def _create_header_cell(test_names: typing.Sequence[str]) -> str:
    toc = _create_test_toc(test_names=test_names)
    return _header_template.format(toc=toc)


def _create_test_toc(test_names: typing.Sequence[str]) -> str:
    line_template = '. [Test: {test_name}](#Test:-{test_name})\n'
    toc = ''
    for t, test_name in enumerate(test_names):
        toc += '    ' + str(t + 1) + line_template.format(test_name=test_name)
    toc = toc.rstrip()
    return _toc_template.format(toc_lines=toc)


def _create_parameters_cell(
    test_paths: typing.Mapping[str, str],
    metrics: typing.Sequence[str],
) -> str:
    parameters = '# parameters\n\n'

    parameters += 'test_paths = ' + (
        repr(test_paths)
        .replace(',', ',\n   ')
        .replace('{', '{\n    ')
        .replace('}', ',\n}')
    )

    parameters += '\n\nmetrics = ' + str(metrics)

    return parameters


def _create_test_chunks(
    test_names: typing.Sequence[str],
) -> typing.Sequence[nbformat.notebooknode.NotebookNode]:
    return [
        cell
        for test_name in test_names
        for cell in _create_test_chunk(test_name)
    ]


def _create_test_chunk(
    test_name: str,
) -> typing.Sequence[nbformat.notebooknode.NotebookNode]:
    from flood.user_io import notebook_io

    return notebook_io.create_cells(
        cell_templates=_test_template_cells,
        inputs={'test_name': test_name},
    )


def print_tests_timing(
    results_payloads: typing.Mapping[str, flood.SingleRunResultsPayload]
) -> None:
    import tooltime
    import toolstr

    time_per_test = {}

    for test_name in results_payloads:
        results = results_payloads[test_name]["results"]
        time_per_test[test_name] = 0.0
        for condition_name in results.keys():
            time = sum(
                item
                for item in results[condition_name]["actual_duration"]
                if item is not None
            ) + sum(
                item
                for item in results[condition_name]["final_wait_time"]
                if item is not None
            )
            time_per_test[test_name] = float(time_per_test[test_name] + time)

    test_timing_rows = []
    t_load_total = 0.0
    for test_name, payload in results_payloads.items():
        t_load_total += payload["t_run_end"] - payload["t_run_start"]
        row = [
            test_name,
            payload["t_run_end"] - payload["t_run_start"],
            time_per_test[test_name],
            tooltime.timestamp_to_iso_pretty(payload["t_run_start"]).replace(
                " ", "\n"
            ),
            tooltime.timestamp_to_iso_pretty(payload["t_run_end"]).replace(
                " ", "\n"
            ),
        ]
        test_timing_rows.append(row)

    total_row = ['TOTAL', t_load_total, sum(time_per_test.values()), None, None]
    test_timing_rows.append(total_row)

    toolstr.print_text_box("Test timing")
    print("(includes setup and cleanup steps)")
    toolstr.print_multiline_table(
        test_timing_rows,
        labels=[
            "test",
            "total\nduration",
            'load test\nduration',
            "t_start",
            "t_end",
        ],
        column_formats={
            'load test\nduration': {'postfix': ' s'},
            'total\nduration': {'postfix': ' s'},
        },
    )


def print_tests_cli_commands(
    results_payloads: typing.Mapping[str, flood.SingleRunResultsPayload]
) -> None:
    import toolstr

    toolstr.print_text_box('CLI commands used for each test')
    for test_name, payload in results_payloads.items():
        toolstr.print_header(test_name)
        args = payload['cli_args']
        args = ['"' + arg + '"' if ' ' in arg else arg for arg in args]
        print(' '.join(args))


def print_tests_nodes(
    results_payloads: typing.Mapping[str, flood.SingleRunResultsPayload]
) -> None:
    import json
    import toolstr

    nodes_per_test = {}
    for test_name, payload in results_payloads.items():
        nodes_per_test[test_name] = payload['nodes']
    n_unique_node_sets = len(
        set(
            json.dumps(nodes, sort_keys=True)
            for nodes in nodes_per_test.values()
        )
    )
    toolstr.print_text_box('Nodes Used')
    if n_unique_node_sets == 1:
        flood.user_io.print_nodes_table(nodes_per_test[test_name])
    else:
        for test_name, nodes_used in nodes_per_test.items():
            flood.user_io.print_header(test_name)
            flood.user_io.print_nodes_table(nodes_used)
            print()


def print_tests_versions(
    results_payloads: typing.Mapping[str, flood.SingleRunResultsPayload]
) -> None:
    import json
    import toolstr

    deps_per_test = {}
    for test_name, payload in results_payloads.items():
        deps_per_test[test_name] = payload['dependency_versions']
    n_unique_flood_versions = len(
        set(payload['flood_version'] for payload in results_payloads.values())
    )
    n_unique_deps_sets = len(
        set(json.dumps(deps, sort_keys=True) for deps in deps_per_test.values())
    )
    toolstr.print_text_box('Versions Used')
    if n_unique_flood_versions == 1 and n_unique_deps_sets == 1:
        print('flood version:', results_payloads[test_name]['flood_version'])
        print()
        toolstr.print_table(
            list(deps_per_test[test_name].items()),
            labels=['dependency', 'version'],
        )
    else:
        print('flood version:', results_payloads[test_name]['flood_version'])
        print()
        for test_name, deps_used in deps_per_test.items():
            flood.user_io.print_header(test_name)
            toolstr.print_table(list(deps_used.items()))
            print()


if typing.TYPE_CHECKING:
    from flood.user_io import notebook_io

_report_template_cells: notebook_io.NotebookTemplate = [
    {
        # header
        'type': 'markdown',
        'f': _create_header_cell,
        'inputs': ['test_names'],
    },
    {
        # imports
        'type': 'code',
        'content': """
            import IPython
            import polars as pl
            import toolstr
            import tooltime

            import flood

            flood.user_io.disable_text_colors()
        """,
        'inputs': [],
    },
    {
        # parameters
        'type': 'code',
        'f': _create_parameters_cell,
        'inputs': ['test_paths', 'metrics'],
    },
    {
        # load data
        'type': 'code',
        'content': """
            # load data

            test_payloads = {{
                test_name: flood.load_single_run_test_payload(test_path)
                for test_name, test_path in test_paths.items()
            }}

            results_payloads = {{
                test_name: flood.load_single_run_results_payload(output_dir=test_path)
                for test_name, test_path in test_paths.items()
            }}
        """,  # noqa: E501
        'inputs': [],
    },
    {
        # Test summary header
        'type': 'markdown',
        'content': '# Test Summary',
        'inputs': [],
    },
    {
        # test list
        'type': 'code',
        'content': """
            # test list

            toolstr.print_text_box('Tests')
            for t, test_name in enumerate(results_payloads.keys()):
                print(str(t + 1) + '.', test_name)
        """,
        'inputs': [],
    },
    {
        # test list
        'type': 'code',
        'content': """
            flood.tests.print_tests_timing(results_payloads)
        """,
        'inputs': [],
    },
    {
        # test list
        'type': 'code',
        'content': """
            flood.tests.print_tests_cli_commands(results_payloads)
        """,
        'inputs': [],
    },
    {
        # test list
        'type': 'code',
        'content': """
            flood.tests.print_tests_nodes(results_payloads)
        """,
        'inputs': [],
    },
    {
        # test list
        'type': 'code',
        'content': """
            flood.tests.print_tests_versions(results_payloads)
        """,
        'inputs': [],
    },
    {
        # Tests header
        'type': 'markdown',
        'content': '# Tests',
        'inputs': [],
    },
    {
        # Tests chunks
        'type': 'chunk',
        'f': _create_test_chunks,
        'inputs': ['test_names'],
    },
]


_test_template_cells: notebook_io.NotebookTemplate = [
    {
        # test header
        'type': 'markdown',
        'content': '# Test: {test_name}',
        'inputs': ['test_name'],
    },
    {
        # load test results
        'type': 'code',
        'content': """
            # load test results

            test_name = '{test_name}'
            test_payload = test_payloads[test_name]
            results_payload = results_payloads[test_name]
            results = results_payload['results']
        """,
        'inputs': ['test_name'],
    },
    {
        # show test metadata
        'type': 'code',
        'content': """
            # show test metadata

            toolstr.print_text_box(test_name + ' parameters')
            test = flood.generate_test(**test_payload['test_parameters'])
            flood.tests.load_tests.print_load_test_summary(test)
            toolstr.print('- nodes tested:')
            nodes_df = pl.from_records(list(results_payload['nodes'].values()))
            toolstr.print_dataframe_as_table(nodes_df)
        """,
        'inputs': [],
    },
    {
        # show result tables
        'type': 'code',
        'content': """
            # show result tables

            flood.user_io.print_metric_tables(results, metrics=metrics, comparison=len(results) == 2)
        """,  # noqa: E501
        'inputs': [],
    },
    {
        # show result figures
        'type': 'code',
        'content': """
            # show result figures

            colors = flood.user_io.get_nodes_plot_colors(nodes=results_payload['nodes'])
            flood.tests.load_tests.plot_load_test_results(
                test_name=test_name,
                outputs=results,
                latency_yscale_log=True,
                colors=colors,
            )
        """,  # noqa: E501
        'inputs': [],
    },
    {
        # show errors
        'type': 'code',
        'content': """
            # show errors

            toolstr.print_text_box('Error messages present in each test')
            unique_errors = {{}}
            for n, name in enumerate(results.keys()):
                unique_errors.setdefault(name, set())
                unique_errors[name] |= {{
                    error for error_list in results[name]['errors'] for error in error_list
                }}
                print(name)
                for error in unique_errors[name]:
                    print('-', error)
                if n != len(results) - 1:
                    print()
        """,  # noqa: E501
        'inputs': [],
    },
    {
        # show complete results
        'type': 'code',
        'content': """
            # show complete results

            for name in results.keys():
                toolstr.print_text_box(name + " Complete Results")
                df = pl.DataFrame(results[name])
                df = df.drop(
                    "status_codes",
                    "errors",
                    "first_request_timestamp",
                    "last_request_timestamp",
                    "last_response_timestamp",
                )
                IPython.display.display(df)
        """,
        'inputs': [],
    },
]

