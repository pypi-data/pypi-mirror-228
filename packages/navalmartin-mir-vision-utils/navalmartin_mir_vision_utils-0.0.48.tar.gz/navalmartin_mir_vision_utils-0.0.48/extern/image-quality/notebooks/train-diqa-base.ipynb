{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --user \"image-quality[dataset]>=1.2.4\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, GlobalAveragePooling2D\n",
    "from notebooks.utils import (\n",
    "    show_images,\n",
    "    gaussian_filter,\n",
    "    image_normalization,\n",
    "    rescale,\n",
    "    image_shape)\n",
    "import imquality.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'tensorflow version {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will implement the Deep CNN-Based Blind Image Quality Predictor (DIQA) methodology proposed by Jongio Kim, Anh-Duc Nguyen, and Sanghoon Lee [1]. Also, I will go through the following TensorFlow 2.0 concepts:\n",
    "- Download and prepare a dataset using a *tf.data.Dataset builder*.\n",
    "- Define a TensorFlow input pipeline to pre-process the dataset records using the *tf.data* API.\n",
    "- Create the CNN model using the *tf.keras* functional API.\n",
    "- Define a custom training loop for the objective error map model.\n",
    "- Train the objective error map and subjective score model.\n",
    "- Use the trained subjective score model to make predictions.\n",
    "\n",
    "*Note: Some of the functions are implemented in [utils.py](https://github.com/ocampor/image-quality/blob/master/notebooks/utils.py) as they are out of the guide's scope.*\n",
    "\n",
    "## What is DIQA?\n",
    "\n",
    "DIQA is an original proposal that focuses on solving some of the most concerning challenges of applying deep learning to image quality assessment (IQA). The advantages against other methodologies are:\n",
    "\n",
    "- The model is not limited to work exclusively with Natural Scene Statistics (NSS) images [1].\n",
    "- Prevents overfitting by splitting the training into two phases (1) feature learning and (2) mapping learned features to subjective scores.\n",
    "\n",
    "## Problem\n",
    "\n",
    "The cost of generating datasets for IQA is high since it requires expert supervision. Therefore, the fundamental IQA benchmarks are comprised of solely a few thousands of records. The latter complicates the creation of deep learning models because they require large amounts of training samples to generalize.\n",
    "\n",
    "As an example, let's consider the most frequently used datasets to train and evaluate IQA methods  [Live](https://live.ece.utexas.edu/research/quality/subjective.htm),  [TID2008](http://www.ponomarenko.info/tid2008.htm), [TID2013](http://www.ponomarenko.info/tid2013.htm), [CSIQ](http://vision.eng.shizuoka.ac.jp/mod/page/view.php?id=23). An overall summary of each dataset is contained in the next table:\n",
    "\n",
    "| Dataset | References | Distortions | Severity | Total Samples |\n",
    "|---------|------------|-------------|----------|---------------|\n",
    "| LiveIQA | 29         | 5           | 5        | 1011          |\n",
    "| TID2008 | 25         | 17          | 5        | 1701          |\n",
    "| TID2013 | 25         | 24          | 5        | 3025          |\n",
    "| CSIQ    | 30         | 6           | 5        | 930           |\n",
    "\n",
    "The total amount of samples does not exceed 4,000 records for any of them.\n",
    "\n",
    "# Dataset\n",
    "\n",
    "The IQA benchmarks only contain a limited amount of records that might not be enough to train a CNN. However, for this guide purpose, we are going to use the [Live](https://live.ece.utexas.edu/research/quality/subjective.htm) dataset. It is comprised of 29 reference images, and 5 different distortions with 5 severity levels each.\n",
    "\n",
    "The first task is to download and prepare the dataset. I have created a couple of TensorFlow dataset builders\n",
    "for image quality assessment and published them in the [image-quality](https://github.com/ocampor/image-quality) package. The builders\n",
    "are an interface defined by [tensorflow-datasets](https://www.tensorflow.org/datasets). \n",
    "\n",
    "*Note: This process might take several minutes because of the size of the dataset (700 megabytes).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "builder = imquality.datasets.LiveIQA()\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After downloading and preparing the data, turn the builder into a dataset, and shuffle it. Note that the batch is equal to 1. The reason is that each image has a different shape. Increasing the batch TensorFlow will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds = builder.as_dataset(shuffle_files=True)['train']\n",
    "ds = ds.shuffle(1024).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output is a generator; therefore, to access it using the bracket operator [ ] causes an error. There are two ways to access the images in the generator. The first way is to turn the generator into an iterator and extract a single sample at a time using the *next* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ds)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output is a dictionary that contains the tensor representation for the distorted image,  the reference image, and the subjective score (dmos).\n",
    "\n",
    "Another way is to extract samples from the generator by taking samples with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in  ds.take(2):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    dmos = tf.round(features['dmos'][0], 2)\n",
    "    distortion = features['distortion'][0]\n",
    "    print(f'The distortion of the image is {dmos} with'\n",
    "          f' a distortion {distortion} and shape {distorted_image.shape}')\n",
    "    show_images([reference_image, distorted_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Methodology\n",
    "\n",
    "## Image Normalization\n",
    "\n",
    "The first step for DIQA is to pre-process the images. The image is converted into grayscale, and then a low-pass filter is applied. The low-pass filter is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{I} = I_{gray} - I^{low}\n",
    "\\end{align*}\n",
    "\n",
    "where the low-frequency image is the result of the following algorithm:\n",
    "\n",
    "1. Blur the grayscale image.\n",
    "2. Downscale it by a factor of 1 / 4.\n",
    "3. Upscale it back to the original size.\n",
    "\n",
    "The main reasons for this normalization are (1) the Human Visual System (HVS) is not sensitive to changes in the low-frequency band, and (2) image distortions barely affect the low-frequency component of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image_low = gaussian_filter(image, 16, 7 / 6)\n",
    "    image_low = rescale(image_low, 1 / 4, method=tf.image.ResizeMethod.BICUBIC)\n",
    "    image_low = tf.image.resize(image_low, size=image_shape(image), method=tf.image.ResizeMethod.BICUBIC)\n",
    "    return image - tf.cast(image_low, image.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in ds.take(2):\n",
    "    distorted_image = features['distorted_image']\n",
    "    reference_image = features['reference_image']\n",
    "    I_d = image_preprocess(distorted_image)\n",
    "    I_d = tf.image.grayscale_to_rgb(I_d)\n",
    "    I_d = image_normalization(I_d, 0, 1)\n",
    "    show_images([reference_image, I_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig 1.** On the left, the original image. On the right, the image after applying the low-pass filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Error Map\n",
    "\n",
    "For the first model, objective errors are used as a proxy to take advantage of the effect of increasing data. The loss function is defined by the mean squared error between the predicted and ground-truth error maps.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = err(\\hat{I}_r, \\hat{I}_d)\n",
    "\\end{align*}\n",
    "\n",
    "and *err(Â·)* is an error function. For this implementation, the authors recommend using\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{e}_{gt} = | \\hat{I}_r -  \\hat{I}_d | ^ p\n",
    "\\end{align*}\n",
    "\n",
    "with *p=0.2*. The latter is to prevent that the values in the error map are small or close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error_map(reference: tf.Tensor, distorted: tf.Tensor, p: float=0.2) -> tf.Tensor:\n",
    "    assert reference.dtype == tf.float32 and distorted.dtype == tf.float32, 'dtype must be tf.float32'\n",
    "    return tf.pow(tf.abs(reference - distorted), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for features in ds.take(3):\n",
    "    reference_image = features['reference_image'] \n",
    "    I_r = image_preprocess(reference_image)\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    e_gt = error_map(I_r, I_d, 0.2)\n",
    "    I_d = image_normalization(tf.image.grayscale_to_rgb(I_d), 0, 1)\n",
    "    e_gt = image_normalization(tf.image.grayscale_to_rgb(e_gt), 0, 1)\n",
    "    show_images([reference_image, I_d, e_gt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig 2.** On the left, the original image. In the middle, the pre-processed image, and finally, the image representation of the error map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Map\n",
    "\n",
    "According to the authors, the model is likely to fail to predict images with homogeneous regions. To prevent it, they propose a reliability function. The assumption is that blurry areas have lower reliability than textured ones. The reliability function is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{r} = \\frac{2}{1 + exp(-\\alpha|\\hat{I}_d|)} - 1\n",
    "\\end{align*}\n",
    "\n",
    "where Î± controls the saturation property of the reliability map. The positive part of a sigmoid is used to assign sufficiently large values to pixels with low intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    assert distorted.dtype == tf.float32, 'The Tensor must by of dtype tf.float32'\n",
    "    return 2 / (1 + tf.exp(- alpha * tf.abs(distorted))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous definition might directly affect the predicted score. Therefore, the average reliability map is used instead.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\hat{r}} = \\frac{1}{\\frac{1}{H_rW_r}\\sum_{(i,j)}\\mathbf{r}(i,j)}\\mathbf{r}\n",
    "\\end{align*}\n",
    "\n",
    "For the Tensorflow function, we just calculate the reliability map and divide it by its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def average_reliability_map(distorted: tf.Tensor, alpha: float) -> tf.Tensor:\n",
    "    r = reliability_map(distorted, alpha)\n",
    "    return r / tf.reduce_mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in ds.take(2):\n",
    "    reference_image = features['reference_image'] \n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    r = average_reliability_map(I_d, 1)\n",
    "    r = image_normalization(tf.image.grayscale_to_rgb(r), 0, 1)\n",
    "    show_images([reference_image, r], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig 3.** On the left, the original image, and on the right, its average reliability map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The loss function is defined as the mean square error of the product between the reliability map and the objective error map. The error is the difference between the predicted error map and the ground-truth error map.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_1(\\hat{I}_d; \\theta_f, \\theta_g) = ||g(f(\\hat{I}_d, \\theta_f), \\theta_g) - \\mathbf{e}_{gt}) \\odot \\mathbf{\\hat{r}}||^2_2\n",
    "\\end{align*}\n",
    "\n",
    "The loss function requires to multiply the error by the reliability map; therefore, we cannot use the default loss implementation *tf.loss.MeanSquareError*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y_true, r):\n",
    "    y_pred = model(x)\n",
    "    return tf.reduce_mean(tf.square((y_true - y_pred) * r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the custom loss, we need to tell TensorFlow how to differentiate it. The good thing is that we can take advantage of [automatic differentiation](https://www.tensorflow.org/tutorials/customization/autodiff) using *tf.GradientTape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(model, x, y_true, r):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, x, y_true, r)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The authors suggested using a Nadam optimizer with a learning rate of *2e-4*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Nadam(learning_rate=2 * 10 ** -4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Objective Error Model\n",
    "For the training phase, it is convenient to utilize the *tf.data* input pipelines to produce a much cleaner and readable code. The only requirement is to create the function to apply to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_map(features):\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    I_r = image_preprocess(features['reference_image'])\n",
    "    r = rescale(average_reliability_map(I_d, 0.2), 1 / 4)\n",
    "    e_gt = rescale(error_map(I_r, I_d, 0.2), 1 / 4)\n",
    "    return (I_d, e_gt, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, map the *tf.data.Dataset* to the *calculate_error_map* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds.map(calculate_error_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the transformation is executed in almost no time. The reason is that the processor is not performing any operation to the data yet, it happens on demand. This concept is commonly called [lazy-evaluation](https://wiki.python.org/moin/Generators).\n",
    "\n",
    "So far, the following components are implemented:\n",
    "1. The generator that pre-processes the input and calculates the target.\n",
    "2. The loss and gradient functions required for the custom training loop.\n",
    "3. The optimizer function.\n",
    "\n",
    "The only missing bits are the models' definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](https://d3i71xaburhd42.cloudfront.net/4b1f961ae1fac044c23c51274d92d0b26722f877/4-Figure2-1.png \"CNN architecture\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig 4.** Architecture of the objective error model and subjective score model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous image, it is depicted how:\n",
    "- The pre-processed image gets into the convolutional neural network (CNN). \n",
    "- It is transformed by 8 convolutions with the Relu activation function and \"same\" padding. This is defined as f(Â·).\n",
    "- The output of f(Â·) is processed by the last convolution with a linear activation function. This is defined as g(Â·)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input = tf.keras.Input(shape=(None, None, 1), batch_size=1, name='original_image')\n",
    "f = Conv2D(48, (3, 3), name='Conv1', activation='relu', padding='same')(input)\n",
    "f = Conv2D(48, (3, 3), name='Conv2', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv3', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv4', activation='relu', padding='same', strides=(2, 2))(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv5', activation='relu', padding='same')(f)\n",
    "f = Conv2D(64, (3, 3), name='Conv6', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv7', activation='relu', padding='same')(f)\n",
    "f = Conv2D(128, (3, 3), name='Conv8', activation='relu', padding='same')(f)\n",
    "g = Conv2D(1, (1, 1), name='Conv9', padding='same', activation='linear')(f)\n",
    "\n",
    "objective_error_map = tf.keras.Model(input, g, name='objective_error_map')\n",
    "\n",
    "objective_error_map.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the custom training loop, it is necessary to:\n",
    "\n",
    "1. Define a metric to measure the performance of the model.\n",
    "2. Calculate the loss and the gradients.\n",
    "3. Use the optimizer to update the weights.\n",
    "4. Print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    epoch_accuracy = tf.keras.metrics.MeanSquaredError()\n",
    "    \n",
    "    step = 0\n",
    "    for I_d, e_gt, r in train:\n",
    "        loss_value, gradients = gradient(objective_error_map, I_d, e_gt, r)\n",
    "        optimizer.apply_gradients(zip(gradients, objective_error_map.trainable_weights))\n",
    "        \n",
    "        epoch_accuracy(e_gt, objective_error_map(I_d))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, epoch_accuracy.result()))\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: It would be a good idea to use the Spearmanâs rank-order correlation coefficient (SRCC) or Pearsonâs linear correlation coefficient (PLCC) as accuracy metrics.*\n",
    "\n",
    "# Subjective Score Model\n",
    "\n",
    "To create the subjective score model, let's use the output of f(Â·) to train a regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = GlobalAveragePooling2D(data_format='channels_last')(f)\n",
    "h = Dense(128, activation='relu')(v)\n",
    "h = Dense(1)(h)\n",
    "subjective_error = tf.keras.Model(input, h, name='subjective_error')\n",
    "\n",
    "subjective_error.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.losses.MeanSquaredError(),\n",
    "    metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "subjective_error.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model with the fit method of *tf.keras.Model* expects a dataset that returns two arguments. The first one is the input, and the second one is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subjective_score(features):\n",
    "    I_d = image_preprocess(features['distorted_image'])\n",
    "    mos = features['dmos']\n",
    "    return (I_d, mos)\n",
    "\n",
    "train = ds.map(calculate_subjective_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, *fit* the subjective score model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = subjective_error.fit(train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predicting with the already trained model is simple. Just use the *predict* method in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(ds))\n",
    "I_d = image_preprocess(sample['distorted_image'])\n",
    "target = sample['dmos'][0]\n",
    "prediction = subjective_error.predict(I_d)[0][0]\n",
    "\n",
    "print(f'the predicted value is: {prediction:.4f} and target is: {target:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this article, we learned how to utilize the tf.data module to create easy to read and memory-efficient data pipelines. Also, we implemented the Deep CNN-Based Blind Image Quality Predictor (DIQA) model using the functional Keras API. The model was trained with a custom training loop that uses the auto differentiation feature from TensorFlow.\n",
    "\n",
    "The next step is to find the hyperparameters that maximize the PLCC or SRCC accuracy metrics and evaluate the overall performance of the model against other methodologies.\n",
    "\n",
    "Another idea is to use a much larger dataset to train the objective error map model and see the resulting overall performance.\n",
    "\n",
    "# Related Articles\n",
    "\n",
    "If you want to learn more about image quality assessment methodologies, you can read.\n",
    "\n",
    "http://bit.ly/advanced-iqa\n",
    "\n",
    "Also, take a look at an image quality assessment method based on natural scene statistics and handcrafted features.\n",
    "\n",
    "http://bit.ly/brisque-article\n",
    "\n",
    "# Jupyter Notebook\n",
    "\n",
    "http://bit.ly/train-diqa-github \n",
    "\n",
    "# Bibliography\n",
    "\n",
    "[1] Kim, J., Nguyen, A. D., & Lee, S. (2019). Deep CNN-Based Blind Image Quality Predictor. IEEE Transactions on Neural Networks and Learning Systems. https://doi.org/10.1109/TNNLS.2018.2829819"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
